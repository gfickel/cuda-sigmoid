{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d435bae-c85f-4368-8cd5-0eff0928458e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.8.3)\n",
      "Requirement already satisfied: wurlitzer in /opt/conda/lib/python3.10/site-packages (3.0.3)\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (1.11.1.1)\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (0.59.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba) (0.42.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib wurlitzer ninja numba numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba3631e-5016-40f4-bd46-cc91e7509f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,math,sys,re\n",
    "import numpy as np\n",
    "import torch\n",
    "from types import SimpleNamespace as ns\n",
    "from collections import namedtuple\n",
    "\n",
    "from utils import show_img,load_cuda,cuda_begin,cdiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14e41709-f1f3-40c1-aa20-bd19737f3d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2, linewidth=140)\n",
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a76aa950-6f87-452d-b048-82da11be0b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdiv(a,b):\n",
    "    \"Int ceiling division of `a` over `b`\"\n",
    "    return (a+b-1)//b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "994f69fd-3989-46e2-ad84-71b7450f1b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext wurlitzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed0d99b2-bf34-4e9c-99e5-0ebf4e4b02d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "# os.environ['NUMBA_ENABLE_CUDASIM'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "import torch\n",
    "\n",
    "@cuda.jit\n",
    "def sigmoid_forward(input, input_len, out):\n",
    "    cbi,cbd,tid = cuda.blockIdx,cuda.blockDim,cuda.threadIdx\n",
    "    idx = cbi.x * cbd.x + tid.x\n",
    "\n",
    "    if idx >= input_len:\n",
    "        return\n",
    "    \n",
    "    if input[idx] >= 0:\n",
    "        res = 1. / ( 1. + math.exp(-input[idx]) )\n",
    "    else:\n",
    "        res = math.exp(input[idx]) / ( 1. + math.exp(input[idx]) )\n",
    "\n",
    "    out[idx] = res\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def sigmoid_backward(input, input_len, out):\n",
    "    cbi,cbd,tid = cuda.blockIdx,cuda.blockDim,cuda.threadIdx\n",
    "    idx = cbi.x * cbd.x + tid.x\n",
    "\n",
    "    if idx >= input_len:\n",
    "        return\n",
    "    \n",
    "    out[idx] = input[idx]*(1-input[idx])\n",
    "\n",
    "\n",
    "def sigmoid_forward_torch(input):\n",
    "    out_tensor = torch.empty_like(input)\n",
    "    positive_mask = input >= 0\n",
    "    out_tensor[positive_mask] = 1. / (1. + torch.exp(-input[positive_mask]))\n",
    "    out_tensor[~positive_mask] = torch.exp(input[~positive_mask]) / (1. + torch.exp(input[~positive_mask]))\n",
    "    \n",
    "    return out_tensor\n",
    "\n",
    "def sigmoid_backward_torch(input):\n",
    "    return input * (1 - input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_numba(input, fun, tw=16, gradcheck=False):\n",
    "    (input_len,) = input.shape\n",
    "    out = torch.zeros(input_len, dtype=torch.float32)\n",
    "    out = out.contiguous().cuda()\n",
    "    tpb = tw\n",
    "    blocks = cdiv(input_len,tpb)\n",
    "    fun[blocks, tpb](input, input_len, out) \n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 5 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.57, 0.00, 1.00, 0.62, 0.38], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input = torch.as_tensor([0.3, -100000, 100000, 0.5, -0.5], dtype=torch.float32)\n",
    "input = input.contiguous().cuda()\n",
    "\n",
    "res = sigmoid_numba(input, sigmoid_forward, 1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 5 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.24, 0.00, 0.00, 0.24, 0.24], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = sigmoid_numba(res, sigmoid_backward, 1)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface for Torch Gradtest\n",
    "\n",
    "Pytorch has an awesome function that does numerical differentiation and checks if our custom backward pass is correct. To use it we must encapsulate our code within torch's autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradcheckSigmoid(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, data: torch.Tensor) -> torch.Tensor:\n",
    "        result = sigmoid_forward_torch(data)\n",
    "        ctx.save_for_backward(result)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor) -> torch.Tensor:\n",
    "        (result,) = ctx.saved_tensors\n",
    "        grad = sigmoid_backward_torch(result)\n",
    "        return grad_output * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradcheck successful\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "sigmoid = GradcheckSigmoid.apply\n",
    "data = torch.randn(4, dtype=torch.double, requires_grad=True)\n",
    "\n",
    "# `torch.autograd.gradcheck` takes a tuple of tensors as input, check if your gradient evaluated\n",
    "# with these tensors are close enough to numerical approximations and returns `True` if they all\n",
    "# verify this condition.\n",
    "if torch.autograd.gradcheck(sigmoid, data, eps=1e-8, atol=1e-7):\n",
    "    print(\"gradcheck successful\")\n",
    "else:\n",
    "    print(\"gradcheck unsuccessful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting NUMBA code to C CUDA\n",
    "\n",
    "I've used the following request to chat GPT 3.5: \"Convert the following 2 CUDA numba python functions to 2 C CUDA functions. Do the minimal amount of changes.\"\n",
    "\n",
    "And then added the sigmoid_forward and sigmoid_backward python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_fwd_src = cuda_begin + r\"\"\"\n",
    "#include <math.h>\n",
    "\n",
    "__global__ void sigmoid_forward_cuda_kernel(const float* input, int input_len, float* out) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (idx < input_len) {\n",
    "        float res;\n",
    "        if (input[idx] >= 0) {\n",
    "            res = 1. / (1. + expf(-input[idx]));\n",
    "        } else {\n",
    "            res = expf(input[idx]) / (1. + expf(input[idx]));\n",
    "        }\n",
    "\n",
    "        out[idx] = res;\n",
    "    }\n",
    "}\n",
    "\n",
    "torch::Tensor sigmoid_forward_cuda(torch::Tensor input) {\n",
    "    CHECK_INPUT(input);\n",
    "    // Get the data pointers and sizes\n",
    "    float* input_data_ptr = input.data_ptr<float>();\n",
    "    int input_len = input.numel();\n",
    "\n",
    "    // Allocate output tensor on GPU\n",
    "    torch::Tensor out_tensor = torch::empty_like(input);\n",
    "\n",
    "    // Get the data pointer for the output tensor\n",
    "    float* out_data_ptr = out_tensor.data_ptr<float>();\n",
    "\n",
    "    // Set block and grid dimensions\n",
    "    int threads_per_block = 256; // You may adjust this based on your specific GPU capabilities\n",
    "    int num_blocks = (input_len + threads_per_block - 1) / threads_per_block;\n",
    "\n",
    "    // Launch CUDA kernel\n",
    "    sigmoid_forward_cuda_kernel<<<num_blocks, threads_per_block>>>(input_data_ptr, input_len, out_data_ptr);\n",
    "\n",
    "    // Synchronize to ensure the kernel is done before proceeding\n",
    "    cudaDeviceSynchronize();\n",
    "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
    "\n",
    "    return out_tensor;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_bwd_src = cuda_begin + r\"\"\"\n",
    "__global__ void sigmoid_backward_cuda_kernel(const float* input, int input_len, float* out) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (idx < input_len) {\n",
    "        out[idx] = input[idx] * (1 - input[idx]);\n",
    "    }\n",
    "}\n",
    "\n",
    "torch::Tensor sigmoid_backward_cuda(torch::Tensor input_tensor) {\n",
    "    // Ensure the input is a float tensor on the GPU\n",
    "    torch::Tensor input_tensor_cuda = input_tensor.cuda().to(torch::kFloat);\n",
    "\n",
    "    // Get the data pointers and sizes\n",
    "    const float* input_data_ptr = input_tensor_cuda.data_ptr<float>();\n",
    "    int input_len = input_tensor_cuda.numel();\n",
    "\n",
    "    // Allocate output tensor on GPU\n",
    "    torch::TensorOptions options = torch::TensorOptions().device(torch::kCUDA).dtype(torch::kFloat);\n",
    "    torch::Tensor out_tensor = torch::empty({input_len}, options);\n",
    "\n",
    "    // Get the data pointer for the output tensor\n",
    "    float* out_data_ptr = out_tensor.data_ptr<float>();\n",
    "\n",
    "    // Set block and grid dimensions\n",
    "    int threads_per_block = 256; // You may adjust this based on your specific GPU capabilities\n",
    "    int num_blocks = (input_len + threads_per_block - 1) / threads_per_block;\n",
    "\n",
    "    // Launch CUDA kernel\n",
    "    sigmoid_backward_cuda_kernel<<<num_blocks, threads_per_block>>>(input_data_ptr, input_len, out_data_ptr);\n",
    "\n",
    "    // Synchronize to ensure the kernel is done before proceeding\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
    "    return out_tensor;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'sigmoid_forward_cuda'\n",
    "cpp_src = 'torch::Tensor sigmoid_forward_cuda(torch::Tensor input);'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_forward = load_cuda(cuda_fwd_src, cpp_src, [fname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.57, 0.00, 1.00, 0.62, 0.38], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = module_forward.sigmoid_forward_cuda(input)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'sigmoid_backward_cuda'\n",
    "cpp_src = 'torch::Tensor sigmoid_backward_cuda(torch::Tensor input);'\n",
    "# cuda_bwd_src\n",
    "module_backward = load_cuda(cuda_bwd_src, cpp_src, [fname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.24, 0.00, 0.00, 0.24, 0.24], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = module_backward.sigmoid_backward_cuda(res)\n",
    "grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check our CUDA Sigmoid gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradcheckSigmoid(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, data: torch.Tensor) -> torch.Tensor:\n",
    "        result = module_forward.sigmoid_forward_cuda(data)\n",
    "        ctx.save_for_backward(result)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor) -> torch.Tensor:\n",
    "        (result,) = ctx.saved_tensors\n",
    "        grad = module_backward.sigmoid_backward_cuda(result)\n",
    "        return grad_output * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradcheck successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/autograd/gradcheck.py:915: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "sigmoid = GradcheckSigmoid.apply\n",
    "data = torch.randn(4, dtype=torch.float, requires_grad=True).contiguous().cuda()\n",
    "\n",
    "# Changing eps and atol since we are dealing with float32\n",
    "if torch.autograd.gradcheck(sigmoid, data, eps=5e-4, atol=1e-7):\n",
    "    print('gradcheck successful')\n",
    "else:\n",
    "    print('gradcheck unsuccessful')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
